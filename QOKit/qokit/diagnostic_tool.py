# enhanced_diagnostic_tool.py
import os
import json
import ast
from pathlib import Path
import inspect
import importlib.util
from typing import Dict, List, Tuple, Set, Optional
import re
from collections import defaultdict
import networkx as nx

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ interconnect.py –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
import sys
sys.path.append('/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/')

try:
    from interconnect import Interconnect, get_component_info, get_stats
except ImportError:
    print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å interconnect.py")
    Interconnect = None

class EnhancedDiagnosticTool:
    def __init__(self, framework_dirs: List[str], new_files_dirs: List[str]):
        self.interconnect = Interconnect() if Interconnect else None
        self.framework_dirs = [Path(d) for d in framework_dirs]
        self.new_files_dirs = [Path(d) for d in new_files_dirs]
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
        self.framework_components = {}
        self.new_components = {}
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        self.dependency_graph = nx.DiGraph()
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ –æ—Ç—á–µ—Ç—ã
        self.integration_recommendations = []
        self.compatibility_report = {}
        self.exclusion_list = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–≤
        self.quantum_keywords = {
            'qaoa': ['qaoa', 'quantum approximate optimization', 'variational quantum'],
            'parameters': ['theta', 'gamma', 'beta', 'params', 'parameters'],
            'optimization': ['optimize', 'minimize', 'cost', 'objective', 'energy'],
            'neural': ['neural', 'network', 'predict', 'train', 'model', 'mlp', 'mlmvn'],
            'quantum_gates': ['gate', 'circuit', 'qubit', 'hamiltonian'],
            'algorithms': ['algorithm', 'solver', 'iteration', 'convergence']
        }

    def scan_framework_components(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        print("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞...")
        
        for directory in self.framework_dirs:
            if not directory.exists():
                print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
                continue
                
            print(f"üìÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {directory}")
            
            for py_file in directory.rglob("*.py"):
                if py_file.name in ["__init__.py", "interconnect.py", "diagnostic_tool.py", "enhanced_diagnostic_tool.py"]:
                    continue
                    
                try:
                    component_info = self._analyze_python_file(py_file, is_framework=True)
                    if component_info:
                        self.framework_components[py_file.stem] = component_info
                        print(f"  ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {py_file.name}")
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {py_file.name}: {e}")

    def scan_new_components(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        print("\nüîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        for directory in self.new_files_dirs:
            if not directory.exists():
                print(f"‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory}")
                continue
                
            print(f"üìÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {directory}")
            
            for py_file in directory.rglob("*.py"):
                if py_file.name == "__init__.py":
                    continue
                    
                try:
                    component_info = self._analyze_python_file(py_file, is_framework=False)
                    if component_info:
                        self.new_components[py_file.stem] = component_info
                        print(f"  ‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {py_file.name}")
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {py_file.name}: {e}")

    def _analyze_python_file(self, py_file: Path, is_framework: bool = True) -> Optional[Dict]:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ Python —Ñ–∞–π–ª–∞"""
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü–∞—Ä—Å–∏–Ω–≥ AST –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            try:
                tree = ast.parse(content)
            except SyntaxError:
                print(f"  ‚ö†Ô∏è  –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ {py_file.name}")
                return None
            
            # –ê–Ω–∞–ª–∏–∑ –∏–º–ø–æ—Ä—Ç–æ–≤
            imports = self._extract_imports(tree)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π
            classes = self._extract_classes(tree)
            functions = self._extract_functions(tree)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords_found = self._find_keywords(content)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            dependencies = self._analyze_dependencies(content, imports)
            
            return {
                'file_path': str(py_file),
                'relative_path': py_file.name,
                'directory': str(py_file.parent),
                'is_framework': is_framework,
                'imports': imports,
                'classes': classes,
                'functions': functions,
                'keywords': keywords_found,
                'dependencies': dependencies,
                'content_hash': hash(content),
                'file_size': py_file.stat().st_size,
                'category': self._categorize_component(classes, functions, keywords_found)
            }
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ {py_file}: {e}")
            return None

    def _extract_imports(self, tree: ast.AST) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∏–º–ø–æ—Ä—Ç–æ–≤ –∏–∑ AST"""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                imports.extend([alias.name for alias in node.names])
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ''
                imports.extend([f"{module}.{alias.name}" for alias in node.names])
        return imports

    def _extract_classes(self, tree: ast.AST) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∞—Å—Å–∞—Ö"""
        classes = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                classes.append({
                    'name': node.name,
                    'methods': methods,
                    'bases': [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases],
                    'docstring': ast.get_docstring(node) or ""
                })
        return classes

    def _extract_functions(self, tree: ast.AST) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ—É–Ω–∫—Ü–∏—è—Ö"""
        functions = []
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                args = [arg.arg for arg in node.args.args]
                functions.append({
                    'name': node.name,
                    'args': args,
                    'docstring': ast.get_docstring(node) or ""
                })
        return functions

    def _find_keywords(self, content: str) -> Dict[str, List[str]]:
        """–ü–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º —Ñ–∞–π–ª–∞"""
        content_lower = content.lower()
        found_keywords = {}
        
        for category, keywords in self.quantum_keywords.items():
            found = []
            for keyword in keywords:
                if keyword in content_lower:
                    found.append(keyword)
            if found:
                found_keywords[category] = found
                
        return found_keywords

    def _analyze_dependencies(self, content: str, imports: List[str]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞"""
        dependencies = {
            'numpy': any('numpy' in imp or 'np' in imp for imp in imports),
            'networkx': any('networkx' in imp or 'nx' in imp for imp in imports),
            'quantum_libs': any(lib in ' '.join(imports) for lib in ['qiskit', 'cirq', 'pennylane']),
            'ml_libs': any(lib in ' '.join(imports) for lib in ['torch', 'tensorflow', 'sklearn']),
            'optimization': any(lib in ' '.join(imports) for lib in ['scipy.optimize', 'cvxpy']),
            'interconnect': 'interconnect' in ' '.join(imports)
        }
        return dependencies

    def _categorize_component(self, classes: List[Dict], functions: List[Dict], keywords: Dict) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        if 'neural' in keywords:
            return 'neural_network'
        elif 'qaoa' in keywords:
            return 'qaoa_algorithm'
        elif 'optimization' in keywords:
            return 'optimization'
        elif 'quantum_gates' in keywords:
            return 'quantum_circuit'
        elif any('test' in cls['name'].lower() for cls in classes):
            return 'test'
        elif any('benchmark' in func['name'].lower() for func in functions):
            return 'benchmark'
        else:
            return 'utility'

    def analyze_benchmark_file(self, benchmark_file: str) -> Set[str]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –±–µ–Ω—á–º–∞—Ä–∫–∞: {benchmark_file}")
        
        active_components = set()
        
        try:
            with open(benchmark_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            for component_name, component_info in self.framework_components.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if component_name in content:
                    active_components.add(component_name)
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
                for cls in component_info['classes']:
                    if cls['name'] in content:
                        active_components.add(component_name)
                        
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
                for func in component_info['functions']:
                    if func['name'] in content:
                        active_components.add(component_name)
            
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {len(active_components)}")
            return active_components
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
            return set()

    def generate_integration_recommendations(self, benchmark_file: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        print("\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏...")
        
        active_components = self.analyze_benchmark_file(benchmark_file)
        
        for new_name, new_info in self.new_components.items():
            recommendations = {
                'new_file': new_info['file_path'],
                'new_component': new_name,
                'category': new_info['category'],
                'recommended_integrations': [],
                'compatibility_score': {},
                'integration_type': self._determine_integration_type(new_info),
                'required_modifications': []
            }
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–∞–∂–¥—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
            for fw_name, fw_info in self.framework_components.items():
                compatibility = self._calculate_compatibility(new_info, fw_info)
                
                if compatibility['score'] > 0.3:  # –ü–æ—Ä–æ–≥ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    recommendations['recommended_integrations'].append({
                        'framework_file': fw_info['file_path'],
                        'component_name': fw_name,
                        'compatibility_score': compatibility['score'],
                        'integration_reasons': compatibility['reasons'],
                        'is_active_in_benchmark': fw_name in active_components
                    })
                    
                recommendations['compatibility_score'][fw_name] = compatibility['score']
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            recommendations['recommended_integrations'].sort(
                key=lambda x: (x['is_active_in_benchmark'], x['compatibility_score']), 
                reverse=True
            )
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π
            recommendations['required_modifications'] = self._suggest_modifications(new_info)
            
            self.integration_recommendations.append(recommendations)

    def _calculate_compatibility(self, new_info: Dict, fw_info: Dict) -> Dict:
        """–†–∞—Å—á–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É –Ω–æ–≤—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞"""
        score = 0.0
        reasons = []
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–æ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º
        dep_overlap = set(new_info['dependencies'].keys()) & set(fw_info['dependencies'].keys())
        if dep_overlap:
            score += 0.2 * len(dep_overlap)
            reasons.append(f"–û–±—â–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {', '.join(dep_overlap)}")
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for category, keywords in new_info['keywords'].items():
            if category in fw_info['keywords']:
                overlap = set(keywords) & set(fw_info['keywords'][category])
                if overlap:
                    score += 0.3 * len(overlap)
                    reasons.append(f"–û–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ({category}): {', '.join(overlap)}")
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if new_info['category'] == 'neural_network' and fw_info['category'] in ['qaoa_algorithm', 'optimization']:
            score += 0.4
            reasons.append("–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–æ–≤—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤")
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø–æ –º–µ—Ç–æ–¥–∞–º/—Ñ—É–Ω–∫—Ü–∏—è–º
        new_methods = set(method['name'] for cls in new_info['classes'] for method in cls['methods'])
        fw_methods = set(method['name'] for cls in fw_info['classes'] for method in cls['methods'])
        
        method_overlap = new_methods & fw_methods
        if method_overlap:
            score += 0.1 * len(method_overlap)
            reasons.append(f"–û–±—â–∏–µ –º–µ—Ç–æ–¥—ã: {', '.join(list(method_overlap)[:3])}")
        
        return {'score': min(score, 1.0), 'reasons': reasons}

    def _determine_integration_type(self, component_info: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if component_info['category'] == 'neural_network':
            return 'predictor'  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–∞–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        elif component_info['category'] == 'optimization':
            return 'optimizer'  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        elif component_info['category'] == 'qaoa_algorithm':
            return 'algorithm'  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º
        else:
            return 'utility'  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–∞–∫ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç

    def _suggest_modifications(self, component_info: Dict) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–π –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        modifications = []
        
        if not component_info['dependencies'].get('interconnect', False):
            modifications.append("–î–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç interconnect")
        
        if component_info['category'] == 'neural_network':
            modifications.append("–î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ predict_parameters() –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å QAOA")
            modifications.append("–î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ on_broadcast() –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π")
        
        if not any('__init__' in method['name'] for cls in component_info['classes'] for method in cls['methods']):
            modifications.append("–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–∞")
        
        return modifications

    def generate_exclusion_list(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤, —Å –∫–æ—Ç–æ—Ä—ã–º–∏ –ù–ï –Ω—É–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å"""
        print("\nüö´ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π...")
        
        exclusion_categories = ['test', 'benchmark', 'utility']
        
        for fw_name, fw_info in self.framework_components.items():
            should_exclude = False
            reasons = []
            
            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if fw_info['category'] in exclusion_categories:
                should_exclude = True
                reasons.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {fw_info['category']}")
            
            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            if any(keyword in fw_info['relative_path'].lower() 
                   for keyword in ['test', 'benchmark', 'example', 'demo']):
                should_exclude = True
                reasons.append("–ò–º—è —Ñ–∞–π–ª–∞ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–µ—Å—Ç/–±–µ–Ω—á–º–∞—Ä–∫/–¥–µ–º–æ")
            
            # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π
            if not fw_info['classes'] and not fw_info['functions']:
                should_exclude = True
                reasons.append("–ù–µ—Ç –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
            
            if should_exclude:
                self.exclusion_list.append({
                    'file': fw_info['file_path'],
                    'component': fw_name,
                    'reasons': reasons
                })

    def print_detailed_report(self):
        """–í—ã–≤–æ–¥ –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        print("\n" + "="*80)
        print("üîç –ü–û–î–†–û–ë–ù–´–ô –û–¢–ß–ï–¢ –ü–û –ò–ù–¢–ï–ì–†–ê–¶–ò–ò –ö–û–ú–ü–û–ù–ï–ù–¢–û–í")
        print("="*80)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø:")
        print(f"  ‚Ä¢ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞: {len(self.framework_components)}")
        print(f"  ‚Ä¢ –ù–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {len(self.new_components)}")
        print(f"  ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {len(self.integration_recommendations)}")
        print(f"  ‚Ä¢ –§–∞–π–ª–æ–≤ –≤ —Å–ø–∏—Å–∫–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π: {len(self.exclusion_list)}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        print("-" * 60)
        
        for rec in self.integration_recommendations:
            print(f"\nüìÑ –ù–û–í–´–ô –§–ê–ô–õ: {rec['new_component']}")
            print(f"   –ü—É—Ç—å: {rec['new_file']}")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {rec['category']}")
            print(f"   –¢–∏–ø –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {rec['integration_type']}")
            
            if rec['recommended_integrations']:
                print(f"   \n   üîó –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
                for i, integration in enumerate(rec['recommended_integrations'][:5], 1):
                    status = "üî• –ê–ö–¢–ò–í–ï–ù" if integration['is_active_in_benchmark'] else "üí§ –ù–ï–ê–ö–¢–ò–í–ï–ù"
                    print(f"     {i}. {Path(integration['framework_file']).name} "
                          f"({integration['compatibility_score']:.2f}) {status}")
                    print(f"        –ü—Ä–∏—á–∏–Ω—ã: {'; '.join(integration['integration_reasons'])}")
            else:
                print(f"   ‚ö†Ô∏è  –ù–ï –ù–ê–ô–î–ï–ù–û –ü–û–î–•–û–î–Ø–©–ò–• –ò–ù–¢–ï–ì–†–ê–¶–ò–ô")
            
            if rec['required_modifications']:
                print(f"   \n   ‚öôÔ∏è  –ù–ï–û–ë–•–û–î–ò–ú–´–ï –ú–û–î–ò–§–ò–ö–ê–¶–ò–ò:")
                for mod in rec['required_modifications']:
                    print(f"     ‚Ä¢ {mod}")
        
        # –°–ø–∏—Å–æ–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        print(f"\nüö´ –§–ê–ô–õ–´ –î–õ–Ø –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø –ò–ó –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        print("-" * 60)
        
        for exclusion in self.exclusion_list:
            print(f"‚Ä¢ {Path(exclusion['file']).name}")
            print(f"  –ü—Ä–∏—á–∏–Ω—ã: {'; '.join(exclusion['reasons'])}")
        
        # –ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
        print(f"\nüìã –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï:")
        print("-" * 60)
        successful_integrations = sum(1 for rec in self.integration_recommendations 
                                    if rec['recommended_integrations'])
        print(f"‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π: {successful_integrations}/{len(self.integration_recommendations)}")
        print(f"‚Ä¢ –§–∞–π–ª–æ–≤ –∏—Å–∫–ª—é—á–µ–Ω–æ: {len(self.exclusion_list)}")
        
        # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print(f"\nüèÜ –¢–û–ü-5 –ò–ù–¢–ï–ì–†–ê–¶–ò–ô:")
        print("-" * 60)
        all_integrations = []
        for rec in self.integration_recommendations:
            for integration in rec['recommended_integrations']:
                all_integrations.append({
                    'new_file': rec['new_component'],
                    'framework_file': Path(integration['framework_file']).name,
                    'score': integration['compatibility_score'],
                    'active': integration['is_active_in_benchmark']
                })
        
        all_integrations.sort(key=lambda x: (x['active'], x['score']), reverse=True)
        
        for i, integration in enumerate(all_integrations[:5], 1):
            status = "üî•" if integration['active'] else "üí§"
            print(f"{i}. {integration['new_file']} ‚Üí {integration['framework_file']} "
                  f"({integration['score']:.2f}) {status}")

    def save_report_to_file(self, output_file: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª"""
        report = {
            'timestamp': str(pd.Timestamp.now()),
            'framework_components': len(self.framework_components),
            'new_components': len(self.new_components),
            'integration_recommendations': self.integration_recommendations,
            'exclusion_list': self.exclusion_list
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")

    def run_full_analysis(self, benchmark_file: str, output_file: str = None):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        print("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
        print("="*80)
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.scan_framework_components()
        self.scan_new_components()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        self.generate_integration_recommendations(benchmark_file)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
        self.generate_exclusion_list()
        
        # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
        self.print_detailed_report()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        if output_file:
            self.save_report_to_file(output_file)
        
        print("\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    
    # –ü—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º
    framework_dirs = [
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/fur",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/fur/c"
    ]
    
    new_files_dirs = [
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/IterFree_neural_solver/",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/IterFree_spectral_core/",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/MLMVN/",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/Rqaoa_agents/",
        "/content/drive/MyDrive/QOKit_enhanced_MLMVN/Rqaoa_core/"
    ]
    
    benchmark_file = "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/benchmark_new.ipynb"
    output_file = "/content/drive/MyDrive/QOKit_enhanced_MLMVN/QOKit/qokit/integration_report.json"
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    diagnostic = EnhancedDiagnosticTool(framework_dirs, new_files_dirs)
    diagnostic.run_full_analysis(benchmark_file, output_file)


if __name__ == "__main__":
    main()